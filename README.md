Apache Spark (Distributed computing engine)

## üü© 1. Apache Spark Core Concepts
üìå **RDD, DataFrame, Lazy |  fault tolerance mechanisms** /f…îÀêlt/ /Ààt…íl…ôr…ôns/ /Ààmek…ôn…™z…ômz/

| No. | ‚ùìQuestion | ‚úÖ Answer | üìò Notes |
| --- | --- | --- | --- |
| 1 | **What is Apache Spark?** | A distributed computing engine for large-scale data processing. | Supports in-memory computation and APIs in Scala, Python, Java, SQL. |
| 2 | **What is an RDD?** | An immutable, partitionable, distributed collection of objects. | immutable to enhance the stability of parallel computation and simplify fault tolerance mechanisms;    supports transformations like `map`, `filter`, `reduceByKey`. |
| 3 | **What is a DataFrame?** | A distributed table with named columns and typed rows. | Built on RDDs; optimized by Catalyst engine; like a distributed Pandas/DataTable. |
| 4 | **What is a transformation?** | A lazy operation that returns a new RDD or DataFrame. | Examples: `map()`, `filter()`, `groupBy()`. |
| 5 | **What is an action?** | An operation that triggers actual computation and returns results. | Examples: `collect()`, `count()`, `show()`. |
| 6 | **What is lazy evaluation?** | Spark builds a logical DAG of transformations, which is only executed when an action is called. | Enables optimization and fault tolerance. |

```mermaid
flowchart TB
    Session["SparkSession"]
    subgraph APIs ["Data Ingestion APIs"]
      PyAPI["PySpark<br>spark.read.text(...)"]
      SQLAPI["SparkSQL<br>createOrReplaceTempView(...)"]
    end

    Session --> PyAPI
    Session --> SQLAPI

    PyAPI --> DF1["DataFrame<br>Schema: city:String"]
    SQLAPI --> TempView["TempView 'cities_txt'"]

    DF1 --> WithCount["withColumn('count', lit(1))"]
    TempView --> SQL1["SQL:<br>SELECT city, COUNT(*) AS cnt<br>FROM cities_txt GROUP BY city"]

    WithCount --> GroupByDF["GroupBy city<br>agg(sum('count'))"]
    GroupByDF & SQL1 --> Action["Action: show()/collect()"]
    Action --> Driver["Driver<br>receives final result"]

    %% Styling
    style Session  fill:#f0f4c3,stroke:#827717,stroke-width:2px
    style PyAPI    fill:#bbdefb,stroke:#0d47a1,stroke-width:2px
    style SQLAPI   fill:#d1c4e9,stroke:#4a148c,stroke-width:2px
    style DF1      fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    style WithCount fill:#e1f5fe,stroke:#039be5,stroke-width:2px
    style GroupByDF fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style TempView fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style SQL1     fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    style Action   fill:#ede7f6,stroke:#5e35b1,stroke-width:2px
    style Driver   fill:#ffffff,stroke:#999,stroke-width:1px,stroke-dasharray:5 5
```

---
## üü® 2. Execution Model

üìå **Job ‚Üí Stage ‚Üí Task**

| No. | Question | Summary |
| --- | --- | --- |
| 7 | What is a Spark job? | Triggered by action, consists of stages. |
| 8 | What is a stage in Spark? | A set of tasks between shuffles. |
| 9 | What is a task? | Unit of execution on a partition. |
| Stage1 | contains narrow transformations (e.g. `map`, `filter`) that don't require shuffling data. | üëâ It is divided into multiple **Tasks**, each processing one partition (e.g. Partition 0, 1, 2). These tasks run **in parallel**. |
| Stage2 | begins **after** Stage 1 is completed, it involves **shuffle** operations like `reduceByKey` | üëâ It too is broken into **Tasks**, now operating on **shuffled partitions** (e.g. Partition A, B). Again, tasks in this stage run in parallel.Ôºå Once Stage 2 completes, the final result is returned to the **Driver** |


```mermaid
flowchart TD
    A["Action<br>(e.g. collect())"]
    A --> Job["Spark Job"]

    Job --> Stage1["Stage 1<br>No shuffle<br>(e.g. map, filter)"]
    Stage1 --> Stage2["Stage 2<br>After shuffle<br>(e.g. reduceByKey)"]

    Stage1 --> T1["Task 1<br>Partition 0"]
    Stage1 --> T2["Task 2<br>Partition 1"]
    Stage1 --> T3["Task 3<br>Partition 2"]

    Stage2 --> T4["Task 1<br>Partition A"]
    Stage2 --> T5["Task 2<br>Partition B"]

    style A fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    style Job fill:#bbdefb,stroke:#1565c0,stroke-width:2px
    style Stage1 fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style Stage2 fill:#fce4ec,stroke:#ad1457,stroke-width:2px
    style T1 fill:#fffde7,stroke:#f57f17
    style T2 fill:#fffde7,stroke:#f57f17
    style T3 fill:#fffde7,stroke:#f57f17
    style T4 fill:#f3e5f5,stroke:#6a1b9a
    style T5 fill:#f3e5f5,stroke:#6a1b9a
```
   
## üüß 3. Shuffle & Partitioning

üìå **Shuffle = Costly, Wide vs Narrow**

| # | Question | Summary |
| --- | --- | --- |
| 10 | What is a shuffle in Spark? | Data redistribution across partitions. |
| 11 | Why is shuffle expensive? | Disk I/O + network + serialization. |
| 12 | What is the difference between narrow and wide transformations? | Narrow = no shuffle, Wide = shuffle needed. |

```mermaid
flowchart TD
    subgraph InputPartitions["Input Partitions"]
        A0["Partition 0"]
        A1["Partition 1"]
        A2["Partition 2"]
    end

    subgraph NarrowTransformation["Narrow Transformation<br>(e.g. map, filter)"]
        B0["Partition 0"]
        B1["Partition 1"]
        B2["Partition 2"]
    end

    subgraph WideTransformation["Wide Transformation<br>(e.g. reduceByKey, groupByKey)"]
        C0["Partition A"]
        C1["Partition B"]
    end

    A0 --> B0
    A1 --> B1
    A2 --> B2

    B0 -->|Shuffle Write| ShuffleStage
    B1 -->|Shuffle Write| ShuffleStage
    B2 -->|Shuffle Write| ShuffleStage

    ShuffleStage -->|Shuffle Read| C0
    ShuffleStage -->|Shuffle Read| C1

    Note1["üí° Shuffle involves:<br>‚Ä¢ Disk I/O<br>‚Ä¢ Network<br>‚Ä¢ Serialization<br><br>‚ùó Partition count may change!"]
    ShuffleStage -.-> Note1

    %% Styling
    style InputPartitions fill:#f0f4c3,stroke:#827717,stroke-width:2px
    style NarrowTransformation fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style WideTransformation fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style ShuffleStage fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style Note1 fill:#fffde7,stroke:#fbc02d,stroke-width:2px,stroke-dasharray: 5 5
```

üîÅ Task Count Comparison

| Stage    | Task Type         | Count | Description                                                             |
|----------|-------------------|--------|-------------------------------------------------------------------------|
| Stage 1  | Map Tasks          | 3      | Each processes one original input partition.                           |
| Stage 1  | Shuffle Map Tasks  | 3      | Same as above; each writes shuffle files for downstream consumption.   |
| **Stage 2**  | Reduce Tasks       | 2      | **Each fetches its partitioned data from all 3 shuffle files, merges and aggregates.** |

## 4. Data SkewÔºàskewness)

üî• Data Skew in Spark - *Unbalanced data across partitions*

| Category                      | Optimization Methods                                           |
|------------------------------|----------------------------------------------------------------|
| 1Ô∏è‚É£ Skewed Input Files        | Repartitioning, Merging small files, Using more suitable file formats |
| 2Ô∏è‚É£ Skewed Join Keys          | Broadcast Join, Salting, Adaptive Query Execution (AQE)       |
| 3Ô∏è‚É£ Skewed Aggregation        | Salting + Two-stage aggregation                               |
| 4Ô∏è‚É£ General Tuning & SQL Hints| SQL hints, Repartitioning, Adaptive Query Execution (AQE)     |

| # | Question | Summary |
| --- | --- | --- |
| 13 | What is data skew? | Unbalanced data across partitions. |
| 14 | What causes it? | Skewed key distribution. |
| 15 | Why is it bad? | Causes long-running tasks, resource underuse. |
| 16 | How to detect it? | Spark UI (task duration, skewed keys). |
| 17 | What ops are sensitive? | Joins, groupByKey, reduceByKey. |
| 18 | What is salting? | Adding randomness to distribute hot keys. |
| 19 | How does salting+grouping work? | Salt keys ‚Üí aggregate ‚Üí merge. |
| 20 | What is a broadcast join? | Send small table to all workers. |
| 21 | When to use broadcast? | Small tables (<10MB), skewed joins. |
| 22 | What Spark configs help? | `spark.sql.shuffle.partitions`, `autoBroadcastJoinThreshold`. |
| 23 | What is AQE? | Adaptive runtime optimizations (incl. skew fix). |
| 24 | Other methods to handle skew? | Filter hot keys, use approx algorithms, repartition. |

### üü¢ 1. Skewed Input Files

```mermaid
flowchart TD
    A["Input Files<br>City Logs (Text)"] --> B["Spark Read<br>spark.read.text(...)"]
    B --> C1["Partition 0<br>üìÑ 10MB"]
    B --> C2["Partition 1<br>üìÑ 500KB"]
    B --> C3["Partition 2<br>üìÑ 600KB"]
    
    C1 --> D["Skewed Stage Execution<br>‚ùå One task takes much longer"]
    
    D --> E["‚úÖ Solution:<br>‚Ä¢ Repartition<br>‚Ä¢ Combine small files<br>‚Ä¢ Use columnar format (e.g., Parquet)"]

    style A fill:#f0f4c3,stroke:#827717,stroke-width:2px
    style D fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style E fill:#e0f7fa,stroke:#006064,stroke-width:2px
```

### üü£ 2. Skewed Join Keys

```mermaid
flowchart TD
    A["Join Two Tables<br>users JOIN logs<br>ON user_id"]
    A --> B["Skewed Key Detected<br>e.g., user_id = 123 appears 1M times"]
    
    B --> C1["Solution 1: Broadcast Join<br>if one table is small (<10MB)"]
    B --> C2["Solution 2: Salting<br>add random prefix/suffix to skewed keys"]
    B --> C3["Solution 3: AQE<br>Adaptive Query Execution handles skewed joins"]

    style A fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    style B fill:#fce4ec,stroke:#ad1457,stroke-width:2px
    style C1 fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style C2 fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    style C3 fill:#fff3e0,stroke:#e65100,stroke-width:2px
```

### üü° 3. Skewed Aggregation Keys

```mermaid
flowchart TD
    A["Raw DataFrame<br>Logs with city names"]
    A --> B["Aggregation: GROUP BY city"]
    B --> C["Skewed Key Detected<br>e.g., 'Singapore' appears 1M times"]
    
    C --> D["‚úÖ Solution:<br>‚Ä¢ Add salt (e.g., city_1, city_2)<br>‚Ä¢ First stage: group by salted keys<br>‚Ä¢ Second stage: merge original keys"]

    style A fill:#f0f4c3,stroke:#827717,stroke-width:2px
    style B fill:#bbdefb,stroke:#0d47a1,stroke-width:2px
    style C fill:#f8bbd0,stroke:#c2185b,stroke-width:2px
    style D fill:#e0f7fa,stroke:#006064,stroke-width:2px
```

### üîß 4. General Spark Tuning & SQL Optimization

```mermaid
flowchart TD
    A["üî• Task takes long or skewed output"]
    A --> B["Check Spark UI"]
    A --> C["Enable AQE<br>(Adaptive Query Execution)"]
    A --> D["Use SQL Hints<br>e.g., BROADCAST(table), REPARTITION(N)"]
    A --> E["Tune configs:<br>‚Ä¢ spark.sql.shuffle.partitions<br>‚Ä¢ spark.sql.autoBroadcastJoinThreshold"]

    style A fill:#fce4ec,stroke:#ad1457,stroke-width:2px
    style B fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style C fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style D fill:#f1f8e9,stroke:#33691e,stroke-width:2px
    style E fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
```

## ‚öôÔ∏è 5. General Spark Tuning (Skew Indirectly Helpful)

These don't solve skew but improve performance or stability overall.
   
```mermaid
flowchart TD
    A["üî• Spark Optimization Techniques"]

    A --> G1["‚öôÔ∏è Memory Tuning<br>- executor.memory<br>- memoryOverhead"]
    A --> G2["‚öôÔ∏è Parallelism<br>- shuffle partitions"]
    A --> G3["‚öôÔ∏è GC Config<br>- memory fraction<br>- G1GC"]
    A --> G4["‚öôÔ∏è Dynamic Allocation<br>- auto executor scaling"]
    A --> G5["‚öôÔ∏è Caching<br>- persist()/cache()"]

    style A fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style G1 fill:#e0f7fa,stroke:#0288d1,stroke-width:2px
    style G2 fill:#e0f7fa,stroke:#0288d1,stroke-width:2px
    style G3 fill:#e0f7fa,stroke:#0288d1,stroke-width:2px
    style G4 fill:#e0f7fa,stroke:#0288d1,stroke-width:2px
    style G5 fill:#e0f7fa,stroke:#0288d1,stroke-width:2px
```

1. **Memory Configuration**
    - Adjust executor/driver memory
    - Configure memory overhead
2. **Parallelism Settings**
    - Tune `spark.sql.shuffle.partitions`
    - Adjust `spark.default.parallelism`
3. **GC Optimization**
    - Tune memory fraction
4. **Dynamic Resource Allocation**
    - Enable auto-scaling of executors
5. **Caching & Persistence**
    - Reuse intermediate results
